{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning, HW 1 Template\n",
    "This document gives a suggested outline for the coding assignment.  Please see the assignment pdf for a more complete description of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Aaron Williams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange         \n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Algorithmic Implementation of a Multi-Class Logistic Regression without Tensorflow (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succintly, for this problem we have an input image that we have vectorized to have $p=784$ features, and the output space is $C=10$ dimensional.  To get the full details on logistic regression, please visit the example codes and and the lectures.  Succinctly, the multiclass logistic regression model is as follows:\n",
    "\n",
    "$$\\mathbf{\\gamma}=\\mathbf{W}\\mathbf{x}+\\mathbf{b},\\quad \\mathbf{W}\\in\\mathbb{R}^{C\\times p},\\quad \\mathbf{b}\\in\\mathbb{R}^{C}$$\n",
    "$$ p(y=j)=\\text{softmax}(\\mathbf{\\gamma})_j$$\n",
    "$$\\ell(y,\\gamma)=\\sum_{j=1}^C1_{(y=j)}\\log(\\text{softmax}(\\mathbf{\\gamma})_j)$$\n",
    "or, if $\\mathbf{r}$ is a one-hot encoding of $y$, then\n",
    "$$\\ell(r,\\gamma)=\\mathbf{r}\\cdot \\log(\\text{softmax}(\\mathbf{\\gamma}))$$\n",
    "We want to implement this model in more basic codes and learn it to build a better understanding of what's going on before moving to using deep learning toolkits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In is permissable in the context of this problem to use the MNIST dataset and minibatcher from Tensorflow, which should reduce the amount of bespoke coding that you have to do.\n",
    "\n",
    "Note that this function is depreciated, but it will work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-8bf8ae5a5303>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data to make sure that its understood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics# Datas \n",
    "print('Training image data: ', mnist.train.images.shape)\n",
    "print('Testing image data: ', mnist.test.images.shape)\n",
    "print('28 x 28 = ', 28*28)\n",
    "\n",
    "# Example image\n",
    "print('\\nTrain image 1 is labelled one-hot as {0}'.format(mnist.train.labels[1,:]))\n",
    "image = np.reshape(mnist.train.images[1,:],[28,28])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull a new data example from MNIST by the following:\n",
    "\n",
    "Note that the digit will change each time you run this because it is randomly sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train image 1 is labelled one-hot as [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2cf14109d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_data=mnist.train.next_batch(1)\n",
    "# Example image\n",
    "print('\\nTrain image 1 is labelled one-hot as {0}'.format(current_data[1]))\n",
    "image = np.reshape(current_data[0],[28,28])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the functions that you need to define to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gradient(W,b,data):\n",
    "    # calculate the gradient on the data\n",
    "    \n",
    "    grad = np.zeros((10,785))\n",
    "    \n",
    "    W_grad = np.zeros((10,784))\n",
    "    b_grad = np.zeros((10,1))\n",
    "    \n",
    "    batch_size = (data[0].shape)[0]\n",
    "    for i in range(0,batch_size):\n",
    "        z = b + np.dot(W,data[0][i]).reshape((10,1))\n",
    "        #print(\"current data is \" + str(np.dot(W,data[0][i]).shape) + \"and b is \" + str(b.shape))\n",
    "        #print(\"z is \" + str(z))\n",
    "        exp_sum = sum(np.exp(z))\n",
    "        #print(\"exp sum is\" + str(exp_sum))\n",
    "        dydz = ((exp_sum - np.exp(z)) * np.exp(z))/(exp_sum**2)\n",
    "        #print(\"dydz is\" + str(dydz))\n",
    "        dfdy = 2*(np.exp(z)/exp_sum - data[1][i].reshape((10,1)))\n",
    "        #print(\"dfdy is\" + str(dfdy))\n",
    "        final_x = np.concatenate(([1],data[0][i])).reshape((1,785))\n",
    "        #print(\"final x is \" + str(final_x.shape))\n",
    "        grad += (final_x * dydz * dfdy)/batch_size\n",
    "        #print(\"grad is \" + str(grad))\n",
    "    \n",
    "    b_grad = grad[:,0].reshape((10,1))\n",
    "    W_grad = grad[:,1:]\n",
    "        \n",
    "    return W_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_loss(W,b,data):\n",
    "    # calculate the loss\n",
    "    avg_loss = 0\n",
    "        \n",
    "    batch_size = (data[0].shape)[0]\n",
    "    for i in range(0,batch_size):\n",
    "        z = b + np.dot(W,data[0][i]).reshape((10,1))\n",
    "        softmax = np.exp(z)/sum(np.exp(z))\n",
    "        #print(\"soft max is \" + str(softmax))\n",
    "        avg_loss -= np.log(softmax[np.argmax(data[1][i])])/batch_size\n",
    "    \n",
    "    #print(\"avg loss is \" + str(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the stochastic gradient descent optimization loop.  Note that you need to fill in the values to make this work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:43<00:00, 96.32it/s]\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10000 # choose the max number of iterations\n",
    "step_size = .05 # choose your step size\n",
    "W = np.zeros((10,784)) # choose your starting parameters (connection weights)\n",
    "b = np.zeros((10,1)) # choose your starting parameters (biases)\n",
    "training_loss_history=[]\n",
    "\n",
    "#print(mnist.train.images.shape)\n",
    "\n",
    "for iter in trange(0,max_iterations):\n",
    "    current_data = mnist.train.next_batch(100)\n",
    "    # note you need to change this to your preferred data format.\n",
    "    W_grad,b_grad=lr_gradient(W,b,current_data)\n",
    "    training_loss_history.append(\\\n",
    "        lr_loss(W,b,current_data))\n",
    "    W=W-step_size*W_grad\n",
    "    b=b-step_size*b_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be a helpful diagnostic tool to visualize the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cf14176d30>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate both your training loss and accuracy and your validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(W,b,data):\n",
    "    \n",
    "    size = (data[0].shape)[0]\n",
    "\n",
    "    accuracy = 0\n",
    "    for i in range(0,size):\n",
    "        z = b + np.dot(W,data[0][i]).reshape((10,1))\n",
    "        softmax = np.argmax(np.exp(z)/sum(np.exp(z)))\n",
    "        #print(\"soft max is \" + str(softmax))\n",
    "        if softmax == np.argmax(data[1][i]):\n",
    "            accuracy += 1/size\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is: [0.3384761]\n",
      "training accuracy is: 0.9125272727267566\n",
      "test loss is: [0.32414146]\n",
      "test accuracy is: 0.9153999999999155\n"
     ]
    }
   ],
   "source": [
    "# Fill in code here.\n",
    "train_size = mnist.train.images.shape[0]\n",
    "training_data = mnist.train.next_batch(train_size)\n",
    "print(\"training loss is: \" + str(lr_loss(W,b,training_data)))\n",
    "print(\"training accuracy is: \" + str(get_accuracy(W,b,training_data)))\n",
    "\n",
    "test_size = mnist.test.images.shape[0]\n",
    "test_data = mnist.train.next_batch(test_size)\n",
    "print(\"test loss is: \" + str(lr_loss(W,b,test_data)))\n",
    "print(\"test accuracy is: \" + str(get_accuracy(W,b,test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Algorithmic Implementation of a Multi-Class Logistic Regression with Tensorflow (30 Points)\n",
    "As above, but now you are allowed to use tensorflow to perform model learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tSet up a logistic regression network, and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-26957aa92b34>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modeling Definition\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Softmax to probabilities\n",
    "py = tf.nn.softmax(y)\n",
    "\n",
    "# Define labels placeholder\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Create a Session object, initialize all variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:10<00:00, 949.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "\n",
    "# Train the model\n",
    "for i in trange(10000): \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run([train_step], feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9253000020980835\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(py, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "# Close session to finish\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tSet up an MLP with a single hidden layer (you can choose the number of hidden nodes) and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Definition\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# hidden layer\n",
    "W_1 = tf.Variable(tf.random_normal([784, 150]))\n",
    "b_1 = tf.Variable(tf.random_normal([150]))\n",
    "z = tf.matmul(x, W_1) + b_1\n",
    "# Output\n",
    "W_2 = tf.Variable(tf.random_normal([150, 10]))\n",
    "b_2 = tf.Variable(tf.random_normal([10]))\n",
    "y = tf.matmul(tf.nn.sigmoid(z), W_2) + b_2\n",
    "\n",
    "# Softmax to probabilities\n",
    "py = tf.nn.softmax(y)\n",
    "\n",
    "# Define labels placeholder\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(py), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(.5).minimize(cross_entropy)\n",
    "\n",
    "# Create a Session object, initialize all variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "\n",
    "# Train the model\n",
    "for i in trange(10000): \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run([train_step], feed_dict={x: batch_xs, y_: batch_ys}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(py, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "# Close session to finish\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tSet up an MLP with two hidden layers (i.e. lecture 2, slide 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Definition\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# hidden layer 1\n",
    "W_1 = tf.Variable(tf.random_normal([784, 200]))\n",
    "b_1 = tf.Variable(tf.random_normal([200]))\n",
    "z_1 = tf.matmul(x, W_1) + b_1\n",
    "# hidden layer 2\n",
    "W_2 = tf.Variable(tf.random_normal([200, 50]))\n",
    "b_2 = tf.Variable(tf.random_normal([50]))\n",
    "z_2 = tf.matmul(tf.nn.sigmoid(z_1), W_2) + b_2\n",
    "# Output\n",
    "W_3 = tf.Variable(tf.random_normal([50, 10]))\n",
    "b_3 = tf.Variable(tf.random_normal([10]))\n",
    "y = tf.matmul(tf.nn.sigmoid(z_2), W_3) + b_3\n",
    "\n",
    "# Softmax to probabilities\n",
    "py = tf.nn.softmax(y)\n",
    "\n",
    "# Define labels placeholder\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(py), reduction_indices=[1]))\n",
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Create a Session object, initialize all variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:26<00:00, 374.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "\n",
    "for i in trange(10000): \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9388999938964844\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(py, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "# Close session to finish\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Performance Comparison (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tDid your implementations and Tensorflow’s implementations from problems 2 and 3 perform the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They performed about the same with tensorflow's version performing slightly better and at a faster rate.  My validation accuracy for problem 2 was generally ~0.910 and took 1 minute 50 seconds to train.  The tensorflow version only took 10 seconds and achieved an accuracy around ~0.92. Both tests were run with 100 as the batch size and over 10000 cycles with a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tWhat is the validation accuracy from the multi-class logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class logistic regression my validation accuracy is generally between 0.92 and 0.925.  This test was run with 100 as the batch size and over 10000 cycles with a gradient optimizer of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tWhat is the validation accuracy from the multi-class MLP with a single hidden layer?  If you change the number of nodes in the hidden layer, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results for accuracy and time of test given a specific layer size:\n",
    "\n",
    "500: 0.945 accuracy, 50 seconds\n",
    "\n",
    "200: 0.946 accuracy, 23 seconds\n",
    "\n",
    "100: 0.941 accuracy, 16 seconds\n",
    "\n",
    "50: 0.939 accuracy, 15 seconds\n",
    "\n",
    "20: 0.925 accuracy, 11 seconds\n",
    "\n",
    "10: 0.906 accuracy, 9 seconds\n",
    "\n",
    "All tests were run with 100 as the batch size and over 10000 cycles and a gradient optimizer of 0.5.  As you can see, a higher hidden layer size meens better performance, but it also takes much longer to compute.  Potentially, the optimizer value, number of cycles, and batch size could have been used to improve performance.  Additionally, I did not account for oerfitting which also might be a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\tWhat is the validation accuracy from the multi-class MLP with two hidden layer?  If you change the number of nodes in the hidden layers, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results for accuracy and time of test given a specific layer size:\n",
    "\n",
    "500,100: 0.931 accuracy, 56 seconds\n",
    "\n",
    "200,100: 0.942 accuracy, 30 seconds\n",
    "\n",
    "200,20: 0.931 accuracy, 28 seconds\n",
    "\n",
    "100,20: 0.932 accuracy, 16 seconds\n",
    "\n",
    "All tests were run with 100 as the batch size and over 10000 cycles and a gradient optimizer of 0.5.  There is less consistency with regards to improving hidden layer size and improved accuracy of the two layer MLP.  There might be some sweet spot with my particular parameter inputs, but I suspect a higher number of cycles would improve the end accuracy result for this NN.  Additionally, I did not account for oerfitting which also might be a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\tDo you match my reported accuracies (lecture 2, slide 58)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I easily match the logistic regression accuracy and don't quite match the hidden layer NN.  This might be because more parameter values need to be played with to reach those accuracies, or more computer power commited to the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

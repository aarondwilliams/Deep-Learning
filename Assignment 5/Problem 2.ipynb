{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Recurrent Neural Networks (30 points)\n",
    "\n",
    "Here we will work on several problems in recurrent neural networks and NLP. First, you should\n",
    "just play with word embeddings to get a greater understanding of what they are and how they\n",
    "work (and when normalized and unnormalized vectors are better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Mostly for technical reasons (my GPU goes OOM when I try), I'm using the moving dataset with the NLP and the simple dataset with the RNN.  The question answers are located at the bottom of this document.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embedddings = all_embeddings[english_word_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(english_embedddings, axis=1)\n",
    "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {word: i for i, word in enumerate(english_words)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pull our data from the Movie Review .txt\n",
    "\n",
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_ex(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell\n",
    "with open(\"movie-pang02.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_ex(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (i) Train an MLP off of the average word embedding to predict sentiment (as done in class) but\n",
    "    optimize the network settings to maximize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use if running on a GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(.001).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "#Run training\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(200):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc.append(acc)\n",
    "        test_reviews = [sample['x'] for sample in test]\n",
    "        test_labels  = [sample['y'] for sample in test]\n",
    "        test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "        acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "        test_acc.append(acc)\n",
    "        print(epoch)\n",
    "        #print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.794\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "sess.close()\n",
    "print(\"Final accuracy:\", acc)\n",
    "\n",
    "plt.plot(range(len(train_acc)),train_acc,range(len(train_acc)),test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.792\n",
      "acc: 0.8\n",
      "acc: 0.798\n",
      "acc: 0.798\n",
      "acc: 0.794\n"
     ]
    }
   ],
   "source": [
    "#try_l1 = [200,150,100,75,50]\n",
    "#try_l2 = [60,40,20,10,5]\n",
    "#for l1 in try_l1:\n",
    "#for l2 in try_l2\n",
    "for _ in range(5):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Placeholders for input\n",
    "    X = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # Three-layer MLP\n",
    "    h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
    "    logits = tf.layers.dense(h2, 1)\n",
    "    probabilities = tf.sigmoid(logits)\n",
    "\n",
    "    # Loss and metrics\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "    # Training\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    # Initialization of variables\n",
    "    initialize_all = tf.global_variables_initializer() \n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(initialize_all)\n",
    "    for epoch in range(200):\n",
    "        for batch in range(train_batches):\n",
    "            data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "            reviews = [sample['x'] for sample in data]\n",
    "            labels  = [sample['y'] for sample in data]\n",
    "            labels = np.array(labels).reshape([-1, 1])\n",
    "            _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "\n",
    "    test_reviews = [sample['x'] for sample in test]\n",
    "    test_labels  = [sample['y'] for sample in test]\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    sess.close()\n",
    "    print(\"acc:\", acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Moving forward,  let's just use AdamOptimizer and the half the initial sizes the layering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (ii) Train a RNN from the word embeddings to predict sentiment (as done in class) and optimize\n",
    "    the network settings to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pull our data from the Movie Review .txt\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "def convert_line_to_embed(line):\n",
    "    y = int(line[0])\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    #We're going to repull the data, but just use the embeddings\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    return {'y': y, 'x':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell\n",
    "with open(\"movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_embed(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 100\n",
    "\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.LSTMCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.6812529462154145 Acc 0.562209809641149\n",
      "batch 199 Loss 0.6464796067140935 Acc 0.6500341528217551\n",
      "batch 299 Loss 0.6012796949698598 Acc 0.698219791972338\n",
      "batch 399 Loss 0.5341334609809568 Acc 0.7873857811984872\n",
      "batch 499 Loss 0.44009819237975994 Acc 0.8543478564637345\n",
      "batch 599 Loss 0.4228448561653151 Acc 0.8714832393511676\n",
      "batch 699 Loss 0.4338927792414936 Acc 0.8689004927673253\n",
      "batch 799 Loss 0.4490734083564355 Acc 0.8215628439541967\n",
      "batch 899 Loss 0.4738998468075115 Acc 0.8147187175054778\n",
      "batch 999 Loss 0.38936516222584255 Acc 0.867774737048057\n",
      "Epoch 0 Test Accuracy 0.9065155807365439\n",
      "batch 99 Loss 0.24147058774772928 Acc 0.9151625568185527\n",
      "batch 199 Loss 0.2152916207587572 Acc 0.931914050623356\n",
      "batch 299 Loss 0.2622331786745472 Acc 0.8894132668817529\n",
      "batch 399 Loss 0.25106660052047763 Acc 0.9096636870309908\n",
      "batch 499 Loss 0.17573437686811044 Acc 0.9420561437141728\n",
      "batch 599 Loss 0.1609973229044352 Acc 0.9462397499673143\n",
      "batch 699 Loss 0.1736110972727284 Acc 0.94457567394799\n",
      "batch 799 Loss 0.12191478886616089 Acc 0.9587954377002824\n",
      "batch 899 Loss 0.19610870520431334 Acc 0.931845175569353\n",
      "batch 999 Loss 0.1504868717242182 Acc 0.9544096729135572\n",
      "Epoch 1 Test Accuracy 0.9461756373937678\n",
      "batch 99 Loss 0.12990280670061757 Acc 0.942300406509537\n",
      "batch 199 Loss 0.1562656708811395 Acc 0.9205009343736664\n",
      "batch 299 Loss 0.14077115586676026 Acc 0.9533215871131482\n",
      "batch 399 Loss 0.10851600835696663 Acc 0.9642887776432805\n",
      "batch 499 Loss 0.10707147933163147 Acc 0.9802388200853442\n",
      "batch 599 Loss 0.08265125244408746 Acc 0.9838133865069297\n",
      "batch 699 Loss 0.15053176921053912 Acc 0.9499167363288625\n",
      "batch 799 Loss 0.12299855700594116 Acc 0.9605281668217042\n",
      "batch 899 Loss 0.1290830754678228 Acc 0.957671006316542\n",
      "batch 999 Loss 0.09003544665250214 Acc 0.9628026111671595\n",
      "Epoch 2 Test Accuracy 0.9688385269121813\n",
      "batch 99 Loss 0.0706362533115542 Acc 0.9788337813077393\n",
      "batch 199 Loss 0.03762599590661745 Acc 0.992252479416171\n",
      "batch 299 Loss 0.08321575946978317 Acc 0.9643109880484083\n",
      "batch 399 Loss 0.11828964380416854 Acc 0.9504936790457484\n",
      "batch 499 Loss 0.061757446410802205 Acc 0.9818790854332913\n",
      "batch 599 Loss 0.11177599158510257 Acc 0.9686339966997596\n",
      "batch 699 Loss 0.08037450515751825 Acc 0.9762791252607446\n",
      "batch 799 Loss 0.08703438095298066 Acc 0.9643907508161087\n",
      "batch 899 Loss 0.09182452586584658 Acc 0.9767237191927955\n",
      "batch 999 Loss 0.06760456593469576 Acc 0.9875133503757854\n",
      "Epoch 3 Test Accuracy 0.9546742209631728\n",
      "batch 99 Loss 0.09150749078689795 Acc 0.9642268768952736\n",
      "batch 199 Loss 0.057339100984518554 Acc 0.9796560766357434\n",
      "batch 299 Loss 0.0335150160649222 Acc 0.9925534661002983\n",
      "batch 399 Loss 0.03423076832442302 Acc 0.9972743277623215\n",
      "batch 499 Loss 0.05278679750017325 Acc 0.9917525124497205\n",
      "batch 599 Loss 0.11639700694269195 Acc 0.9725906837019777\n",
      "batch 699 Loss 0.08326477040736652 Acc 0.9899673037827365\n",
      "batch 799 Loss 0.08233328450375603 Acc 0.9821548045675993\n",
      "batch 899 Loss 0.08273276031648474 Acc 0.970295598342963\n",
      "batch 999 Loss 0.043136348451183096 Acc 0.9891272283153544\n",
      "Epoch 4 Test Accuracy 0.9745042492917847\n",
      "batch 99 Loss 0.03914354501545048 Acc 0.985462248932347\n",
      "batch 199 Loss 0.035176968304362045 Acc 0.9845238453345618\n",
      "batch 299 Loss 0.018872144323795043 Acc 0.994335226873903\n",
      "batch 399 Loss 0.016715671232467363 Acc 0.9979265098298727\n",
      "batch 499 Loss 0.036694703215150094 Acc 0.9866406265780355\n",
      "batch 599 Loss 0.05759792303255732 Acc 0.9721560067659432\n",
      "batch 699 Loss 0.057677591835480484 Acc 0.9793170427833433\n",
      "batch 799 Loss 0.05466901469099796 Acc 0.9870666864934611\n",
      "batch 899 Loss 0.05124978271444177 Acc 0.982103298711453\n",
      "batch 999 Loss 0.04111533441292772 Acc 0.9849346508153372\n",
      "Epoch 5 Test Accuracy 0.9631728045325779\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(6):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['x'] for sample in data]).reshape([1,-1,n_inputs])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    \n",
    "    #On every epoch    \n",
    "    test_acc=0\n",
    "    n=0\n",
    "    for sample in test:\n",
    "        test_reviews = np.array([sample['x']]).reshape([1,-1,n_inputs])\n",
    "        test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "        test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "        test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "        n+=1\n",
    "    acc=test_acc/n \n",
    "\n",
    "    print(\"Epoch\", epoch, \"Test Accuracy\", acc)\n",
    "        \n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We're going with the LSTM RNN setup.  We're also upping the number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (iii) Encode each vocabulary word as a one-hot vector. Train an MLP on the average of the onehot\n",
    "    vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "list built\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Dataset built  2000\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "#Let's build that one hot encoded dataset\n",
    "#We're runnign the more difficult review set again for the MLP\n",
    "with open(\"movie-pang02.txt\", \"r\", encoding=enc) as f:\n",
    "    \n",
    "    vocab_list = []\n",
    "    all_lines = []\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    #Creates a vocabulary list from the text\n",
    "    for count, line in enumerate(f.readlines()):\n",
    "        labels.append(int(line[0]))\n",
    "        words = line[2:].translate(remove_punct).lower().split()\n",
    "        all_lines.append(words)\n",
    "        for word in words:\n",
    "            if word not in vocab_list:\n",
    "                vocab_list.append(word)\n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "        \n",
    "    print(\"list built\")\n",
    "    one_hots = np.zeros(len(vocab_list))\n",
    "    \n",
    "    #Gets the averaged one_hot encodings\n",
    "    for l, line in enumerate(all_lines):\n",
    "        embeddings = []\n",
    "        for word in line:\n",
    "            one_hots[vocab_list.index(word)] += 1\n",
    "        one_hots = one_hots/sum(one_hots)\n",
    "        dataset.append({'x': one_hots, 'y': labels[l]})\n",
    "        if l%100 == 0:\n",
    "            print(l)\n",
    "    \n",
    "    print(\"Dataset built \",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39856\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:] \n",
    "\n",
    "x_size = len(vocab_list)\n",
    "print(x_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, x_size])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, 50, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 10, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(.001).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  2\n",
      "Epoch  4\n",
      "Epoch  6\n",
      "Epoch  8\n",
      "Epoch  10\n",
      "Epoch  12\n",
      "Epoch  14\n",
      "Epoch  16\n",
      "Epoch  18\n"
     ]
    }
   ],
   "source": [
    "#Run training\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(20):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 2 == 0:\n",
    "        train_acc.append(acc)\n",
    "        test_reviews = [sample['x'] for sample in test]\n",
    "        test_labels  = [sample['y'] for sample in test]\n",
    "        test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "        acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "        test_acc.append(acc)\n",
    "        print(\"Epoch \",epoch)\n",
    "        #print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.888\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "sess.close()\n",
    "print(\"Final accuracy:\", acc)\n",
    "\n",
    "plt.plot(range(len(train_acc)),train_acc,range(len(train_acc)),test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (iv) As in (iii), but use an RNN on the one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "list built\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "Dataset built  1411\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "#Let's build that one hot encoded dataset\n",
    "with open(\"movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    \n",
    "    vocab_list = []\n",
    "    all_lines = []\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for count, line in enumerate(f.readlines()):\n",
    "        labels.append(int(line[0]))\n",
    "        words = line[2:].translate(remove_punct).lower().split()\n",
    "        all_lines.append(words)\n",
    "        for word in words:\n",
    "            if word not in vocab_list:\n",
    "                vocab_list.append(word)\n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "        \n",
    "    print(\"list built\")\n",
    "    one_hots = []\n",
    "    for l, line in enumerate(all_lines):\n",
    "        one_hots = []\n",
    "        for word in line:\n",
    "            one_hot = np.zeros(len(vocab_list))\n",
    "            one_hot[vocab_list.index(word)] += 1\n",
    "            one_hots.append(one_hot)\n",
    "        dataset.append({'x': one_hots, 'y': labels[l]})\n",
    "        if l%100 == 0:\n",
    "            print(l)\n",
    "    \n",
    "    print(\"Dataset built \",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = len(vocab_list)\n",
    "n_neurons = 150\n",
    "\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.LSTMCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.7085689392796922 Acc 0.5199488871510672\n",
      "batch 199 Loss 0.7099679844109034 Acc 0.6304773219950525\n",
      "batch 299 Loss 0.680517211931507 Acc 0.5390357034450857\n",
      "batch 399 Loss 0.6384245186922266 Acc 0.6650029073132011\n",
      "batch 499 Loss 0.5789077688748608 Acc 0.6912880493291421\n",
      "batch 599 Loss 0.5103176220114027 Acc 0.8110125546381033\n",
      "batch 699 Loss 0.4813289842191055 Acc 0.864002278776767\n",
      "batch 799 Loss 0.4284467725482273 Acc 0.8899384459757789\n",
      "batch 899 Loss 0.38156666716436977 Acc 0.9179369719688479\n",
      "batch 999 Loss 0.48323079479364395 Acc 0.850279072189992\n",
      "Epoch 0 Test Accuracy 0.8441926345609065\n",
      "batch 99 Loss 0.4175599475913576 Acc 0.8825376816397188\n",
      "batch 199 Loss 0.36055877656523166 Acc 0.9107705998228416\n",
      "batch 299 Loss 0.29959452528934366 Acc 0.9380816840577643\n",
      "batch 399 Loss 0.24455103003022086 Acc 0.9500754644536813\n",
      "batch 499 Loss 0.19691308109180689 Acc 0.9588918915157679\n",
      "batch 599 Loss 0.20782721747625893 Acc 0.9371406756165866\n",
      "batch 699 Loss 0.21314054746781613 Acc 0.9376926118554901\n",
      "batch 799 Loss 0.2013730613642836 Acc 0.9503815607627801\n",
      "batch 899 Loss 0.16897040751243558 Acc 0.9468976578800877\n",
      "batch 999 Loss 0.13267824832768074 Acc 0.9541319294098487\n",
      "Epoch 1 Test Accuracy 0.9178470254957507\n",
      "batch 99 Loss 0.1005072506090378 Acc 0.977422334479097\n",
      "batch 199 Loss 0.1599468855172014 Acc 0.9787211680345177\n",
      "batch 299 Loss 0.13713210296977849 Acc 0.9785926878358854\n",
      "batch 399 Loss 0.14079038868422894 Acc 0.9703570183166723\n",
      "batch 499 Loss 0.13910725222272133 Acc 0.9738928929646683\n",
      "batch 599 Loss 0.09414884480759897 Acc 0.985395095600116\n",
      "batch 699 Loss 0.07802511471765709 Acc 0.9876196556984432\n",
      "batch 799 Loss 0.041804769376386754 Acc 0.9954683935895327\n",
      "batch 899 Loss 0.03266490037278294 Acc 0.998341285495848\n",
      "batch 999 Loss 0.037409727811004205 Acc 0.9918456639745047\n",
      "Epoch 2 Test Accuracy 0.9348441926345609\n",
      "batch 99 Loss 0.03196041343665679 Acc 0.9938132782456434\n",
      "batch 199 Loss 0.023648764257405693 Acc 0.9977354597514474\n",
      "batch 299 Loss 0.01791732295890393 Acc 0.9991711050309148\n",
      "batch 399 Loss 0.02182001760301187 Acc 0.9932055713501122\n",
      "batch 499 Loss 0.035899047295551524 Acc 0.9937406529044324\n",
      "batch 599 Loss 0.033441797181363905 Acc 0.991019158942071\n",
      "batch 699 Loss 0.15507475920703404 Acc 0.960775612012078\n",
      "batch 799 Loss 0.0722748387455381 Acc 0.9856426054297712\n",
      "batch 899 Loss 0.07969233512245709 Acc 0.9702357755201013\n",
      "batch 999 Loss 0.0869384304532532 Acc 0.9758126008496248\n",
      "Epoch 3 Test Accuracy 0.9348441926345609\n",
      "batch 99 Loss 0.049536083899164064 Acc 0.9850330560125801\n",
      "batch 199 Loss 0.06327002630821255 Acc 0.9806119754746154\n",
      "batch 299 Loss 0.03567916108494441 Acc 0.9929033559903103\n",
      "batch 399 Loss 0.023574917080179985 Acc 0.9974023987779508\n",
      "batch 499 Loss 0.015650419747805174 Acc 0.9990491939429993\n",
      "batch 599 Loss 0.019543894671091442 Acc 0.9936019135614839\n",
      "batch 699 Loss 0.012191617862921115 Acc 0.9976580934412416\n",
      "batch 799 Loss 0.00865900921323057 Acc 0.9991427864592546\n",
      "batch 899 Loss 0.009942666541285991 Acc 0.9996862321207096\n",
      "batch 999 Loss 0.011053915055536627 Acc 0.9998851508085267\n",
      "Epoch 4 Test Accuracy 0.9376770538243626\n"
     ]
    }
   ],
   "source": [
    "#Run training\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(5):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['x'] for sample in data]).reshape([1,-1,n_inputs])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    \n",
    "    #On every epoch    \n",
    "    test_acc=0\n",
    "    n=0\n",
    "    for sample in test:\n",
    "        test_reviews = np.array([sample['x']]).reshape([1,-1,n_inputs])\n",
    "        test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "        test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "        test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "        n+=1\n",
    "    acc=test_acc/n \n",
    "\n",
    "    print(\"Epoch\", epoch, \"Test Accuracy\", acc)\n",
    "        \n",
    "    random.shuffle(train)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (v) Why did the word embeddings work better (hint: the word embeddings will work better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings were established to provide a distributed representation of words in way such that similar words are more closely positioned to each other in a high dimensional space.  This means that similar words bear a similar distribution across  their associated embedded vectors.  \n",
    "\n",
    "If a word  occurs only a few times  across the data passages that we looked at, then we have a very limited association for what that word might mean if just using one-hot vectors.  However, a word may only appear once across all training passages, but by being closely associated with certain words it can be identified as having a particular connotation without every appearing in training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (vi) How does cross-validation change when considering a time-series instead of multiple\n",
    "    instances (as in our movie reviews)? Only a description is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In typical cross-validation, you break up your data into random subsets.  You draw multiple training and test sets from those  groupings in a typically organized fashion.  Unfortunately, when attempting to cross-validate a time-series, you run into the issue that your algorithm is holding information between series.  For that reason, you need to make sure that the items you are testing on are able to hold on to that transition information when applicable.\n",
    "\n",
    "One example I remember given is class is that if a given dataset is full time series, you can break apart the time series into even sections.  You train on the first section, and then test on the second section.  Train on the second section, and test on the third section, and continue in this manner until you've passed through all of the data.  This gives you a good loock at if your RNN is storing appropriate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

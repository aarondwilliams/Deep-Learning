{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenaardvark\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# Use if running on a GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: SARSA Q-Learning (30 points)\n",
    "    \n",
    "SARSA is an alternative way of learning a Q-Value function with a lot of similarity to the simpler\n",
    "policy introduced during the code exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (a) Implement a deep Q-learning approach to the cart pole problem (was done in class)\n",
    "    using an ï¿½-Greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCartPoleSolver_Q():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        #if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                if e % 100 == 0 and not self.quiet:\n",
    "                    self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        self.env.close()\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.85 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 33.62 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 81.67 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 111.88 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 129.2 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 124.85 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 123.8 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 162.28 ticks.\n",
      "Ran 830 episodes. Solved after 730 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "agent = DQNCartPoleSolver_Q()\n",
    "agent.run()\n",
    "del agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (b) Implement a SARSA approach with a deep network to solve the cart pole problem using\n",
    "    an e-Greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCartPoleSolver_SARSA():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        #if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for counter, [state, action, reward, next_state, done] in enumerate(minibatch):\n",
    "            if counter+1 < len(minibatch):\n",
    "                y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "                next_action = minibatch[counter+1][1]\n",
    "                #print(self.sess.run(self.Q, feed_dict={self.state_: next_state}))\n",
    "                y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state})[0][next_action]\n",
    "                x_batch.append(state[0])\n",
    "                y_batch.append(y_target[0])\n",
    "        \n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.env.env.close()\n",
    "        self.env.close()\n",
    "        \n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                if e % 100 == 0 and not self.quiet:\n",
    "                    self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        self.env.close()\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 32.37 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 54.28 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 183.8 ticks.\n",
      "Ran 364 episodes. Solved after 264 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "agent = DQNCartPoleSolver_SARSA()\n",
    "agent.run()\n",
    "del agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (c) Evaluate the impact of gamma in the cart pole problem. How important is this parameter?\n",
    "    How does it affect stability and learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For graph\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 11.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 18.48 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 73.72 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 172.21 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 143.0 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 167.83 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 184.4 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 187.27 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 188.3 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 190.89 ticks.\n",
      "Ran 981 episodes. Solved after 881 trials âœ”\n",
      "... with gamma of 1.0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 14.74 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 74.14 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 114.16 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 159.72 ticks.\n",
      "Ran 455 episodes. Solved after 355 trials âœ”\n",
      "... with gamma of 1.0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 18.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 32.92 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 61.3 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 192.58 ticks.\n",
      "Ran 304 episodes. Solved after 204 trials âœ”\n",
      "... with gamma of 1.0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 35.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 10.26 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 35.95 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 152.12 ticks.\n",
      "Ran 376 episodes. Solved after 276 trials âœ”\n",
      "... with gamma of 0.99\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 26.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 21.69 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 41.8 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 128.37 ticks.\n",
      "Ran 395 episodes. Solved after 295 trials âœ”\n",
      "... with gamma of 0.99\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 10.25 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 28.23 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 126.12 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 185.43 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 185.1 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 188.65 ticks.\n",
      "Ran 609 episodes. Solved after 509 trials âœ”\n",
      "... with gamma of 0.99\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 23.76 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 53.18 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 170.59 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 161.98 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 181.87 ticks.\n",
      "Ran 532 episodes. Solved after 432 trials âœ”\n",
      "... with gamma of 0.9\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 34.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 33.95 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 89.4 ticks.\n",
      "Ran 284 episodes. Solved after 184 trials âœ”\n",
      "... with gamma of 0.9\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 11.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 27.68 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 167.2 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 169.85 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 184.62 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 158.73 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 178.09 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 187.43 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 175.7 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 176.54 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.9\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 41.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 18.24 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 22.61 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 15.06 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 34.14 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 67.24 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 65.88 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 68.55 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 57.77 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 56.98 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.5\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.33 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 25.3 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 23.78 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 36.32 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 60.64 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 108.56 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 99.84 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 87.9 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 94.34 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.5\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 28.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 24.9 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 16.12 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 36.06 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 37.87 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 65.7 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 69.92 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 69.68 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 49.99 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 79.48 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.5\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 22.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.02 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.12 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.22 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.41 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.46 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.38 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.54 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 9.52 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.44 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.1\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 14.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 21.92 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 14.12 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 18.87 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 19.96 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 22.15 ticks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 600] - Mean survival time over last 100 episodes was 25.41 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 32.99 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 20.81 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 19.27 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.1\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 12.81 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.62 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.54 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.54 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.64 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.7 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.6 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 9.65 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.72 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0.1\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 36.26 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 11.86 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 20.32 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 22.57 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 59.19 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 64.18 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 42.45 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 32.39 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 28.18 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.95 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.77 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.51 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.36 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.51 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 9.51 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.37 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.6 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.06 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.87 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 10.29 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.86 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.65 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.7 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 9.7 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.63 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0\n"
     ]
    }
   ],
   "source": [
    "#Let's just iterate over a few different values of gamma and see how many trials it takes each of them\n",
    "#Let's test gamma on the SARSA solver as the results seem a little better\n",
    "\n",
    "gammas = [1.,0.99,0.9,0.5,0.1,0]\n",
    "gamma_eps = []\n",
    "trials = 3\n",
    "\n",
    "for curr_gamma in gammas:\n",
    "    \n",
    "    curr_vals = []\n",
    "    \n",
    "    for _ in range(trials):\n",
    "        curr_agent = DQNCartPoleSolver_SARSA(gamma = curr_gamma)\n",
    "        curr_vals.append(curr_agent.run())\n",
    "        del curr_agent\n",
    "        print(\"... with gamma of {}\".format(curr_gamma))\n",
    "\n",
    "    gamma_eps.append(np.mean(curr_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's show the results over our range of gammas\n",
    "plt.plot(gamma_eps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a graph which shows the trial count average over three tests for a range of epsilon values [1.,0.99,0.9,0.5,0.1,0]. Gamma values seem best at around 0.99, and anything 0.5 and below is just non-functional.  A gamma of 0.99 is also the most consistent completign at around 500 iterations per trial.  Gamma values are best chosen in this case at above 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (d) Evaluate the impact of epsilon on the learning over the feasible range (0-1). What values\n",
    "    seem reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 53.77 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 69.01 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 193.29 ticks.\n",
      "Ran 320 episodes. Solved after 220 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 44.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 42.83 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 101.48 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 160.57 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 168.08 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 144.78 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 134.87 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 164.46 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 162.44 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 162.63 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 18.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 19.86 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 112.59 ticks.\n",
      "Ran 276 episodes. Solved after 176 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 15.4 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 44.19 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 114.23 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 135.96 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 165.26 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 187.88 ticks.\n",
      "Ran 624 episodes. Solved after 524 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 39.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 33.68 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 155.62 ticks.\n",
      "Ran 237 episodes. Solved after 137 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 14.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 24.86 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 88.01 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 113.42 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 193.37 ticks.\n",
      "Ran 402 episodes. Solved after 302 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 22.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 33.44 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 87.56 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 144.52 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 168.65 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 168.03 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 165.37 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 182.84 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 186.69 ticks.\n",
      "Ran 870 episodes. Solved after 770 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 18.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 24.98 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 72.49 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 189.3 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 187.42 ticks.\n",
      "Ran 432 episodes. Solved after 332 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 20.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 32.16 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 143.82 ticks.\n",
      "Ran 254 episodes. Solved after 154 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 30.87 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 122.82 ticks.\n",
      "Ran 289 episodes. Solved after 189 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 27.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 13.69 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 38.91 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 191.35 ticks.\n",
      "Ran 312 episodes. Solved after 212 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 11.8 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 33.16 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 138.48 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 190.43 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 185.44 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 173.08 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 170.93 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 177.66 ticks.\n",
      "Ran 831 episodes. Solved after 731 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 200.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 65.57 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 28.13 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 112.46 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 80.1 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 139.92 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 163.18 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 152.65 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 186.21 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 146.09 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 39.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 34.47 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 143.82 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 122.71 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 135.47 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 172.66 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 177.27 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 172.33 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 188.31 ticks.\n",
      "Ran 837 episodes. Solved after 737 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 14.83 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 38.38 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 71.57 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 160.84 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 147.13 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 166.57 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 180.79 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 190.45 ticks.\n",
      "Ran 817 episodes. Solved after 717 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.16 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 42.91 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 188.55 ticks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 400] - Mean survival time over last 100 episodes was 190.13 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 154.31 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 194.01 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 194.38 ticks.\n",
      "Ran 727 episodes. Solved after 627 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 21.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 45.88 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 109.68 ticks.\n",
      "Ran 258 episodes. Solved after 158 trials âœ”\n",
      "... with gamma of 0\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 9.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 18.29 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 58.24 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 39.12 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 185.82 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 187.31 ticks.\n",
      "Ran 511 episodes. Solved after 411 trials âœ”\n",
      "... with gamma of 0\n"
     ]
    }
   ],
   "source": [
    "epsilons = [1.,0.99,0.9,0.5,0.1,0]\n",
    "epsilon_eps = []\n",
    "\n",
    "for curr_epsilon in epsilons:\n",
    "    \n",
    "    curr_vals = []\n",
    "    \n",
    "    for _ in range(trials):\n",
    "        curr_agent = DQNCartPoleSolver_SARSA(epsilon = curr_epsilon)\n",
    "        curr_vals.append(curr_agent.run())\n",
    "        del curr_agent\n",
    "        print(\"... with gamma of {}\".format(curr_epsilon))\n",
    "        \n",
    "    epsilon_eps.append(np.mean(curr_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's show the results over our range of gammas\n",
    "plt.plot(epsilon_eps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a graph which shows the trial count average over three tests for a range of epsilon values [1.,0.99,0.9,0.5,0.1,0].  We can see that epsilon values around 0.99 are the best, though trials aren't conclusive due to their inconsistency.  It seems best to select an epsilon value that around 0.5 or above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (e) Pick one other small agent from the OpenAI gym and apply the same techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountCarSolver_SARSA():\n",
    "    def __init__(self, n_episodes=1000, max_env_steps=250, gamma=0.95, epsilon=0.95, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.5, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        h = tf.layers.dense(self.state_, units=96, activation=tf.nn.relu)\n",
    "        h = tf.layers.dense(h, units=96, activation=tf.nn.relu)\n",
    "        self.Q = tf.layers.dense(h, units=3)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.005, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 2])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for counter, [state, action, reward, next_state, done] in enumerate(minibatch):\n",
    "            if counter+1 < len(minibatch):\n",
    "                y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "                next_action = minibatch[counter+1][1]\n",
    "                #print(self.sess.run(self.Q, feed_dict={self.state_: next_state}))\n",
    "                y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
    "                x_batch.append(state[0])\n",
    "                y_batch.append(y_target[0])\n",
    "        \n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.env.env.close()\n",
    "        self.env.close()\n",
    "        \n",
    "    def run(self):\n",
    "        scores = deque(maxlen=5)\n",
    "        reward_counter = deque(maxlen=50)\n",
    "        \n",
    "        for e in range(self.n_episodes):\n",
    "            \n",
    "            #Stored values for reward\n",
    "            min_pos = 0.6\n",
    "            max_pos = -1.2\n",
    "            max_vel = 0\n",
    "            reward_count = 0\n",
    "            \n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            while not done:\n",
    "                if e % 50 == 0 and not self.quiet:\n",
    "                    self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                #Alter reward to convince the vehicle to try and reach maximum state\n",
    "                if next_state[0] >= 0.5:\n",
    "                    reward = 2\n",
    "                else:\n",
    "                    curr_pos = next_state[0]\n",
    "                    if curr_pos > max_pos:\n",
    "                        max_pos = curr_pos\n",
    "                        reward = 1\n",
    "                    elif curr_pos < min_pos:\n",
    "                        min_pos = curr_pos\n",
    "                        reward = 0.5\n",
    "                    curr_vel = abs(next_state[1])\n",
    "                    if curr_vel > max_vel:\n",
    "                        max_vel = curr_vel\n",
    "                        reward = 0.5\n",
    "                if reward > 0:\n",
    "                    reward_count += reward\n",
    "                \n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "            scores.append(reward)    \n",
    "            reward_counter.append(reward_count)   \n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score == 2 and e >= 5:\n",
    "                self.env.render()\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 5))\n",
    "                return e - 5\n",
    "            if e % 50 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Average count of rewards is {}.'.format(e, np.mean(reward_counter)))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        self.env.close()\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode 0] - Average count of rewards is 66.0.\n",
      "[Episode 50] - Average count of rewards is 54.65.\n",
      "[Episode 100] - Average count of rewards is 61.41.\n",
      "[Episode 150] - Average count of rewards is 50.91.\n",
      "[Episode 200] - Average count of rewards is 55.83.\n",
      "[Episode 250] - Average count of rewards is 57.15.\n",
      "[Episode 300] - Average count of rewards is 55.66.\n",
      "[Episode 350] - Average count of rewards is 50.74.\n",
      "[Episode 400] - Average count of rewards is 53.85.\n",
      "[Episode 450] - Average count of rewards is 56.15.\n",
      "[Episode 500] - Average count of rewards is 55.34.\n",
      "[Episode 550] - Average count of rewards is 48.28.\n",
      "[Episode 600] - Average count of rewards is 62.95.\n",
      "[Episode 650] - Average count of rewards is 57.05.\n",
      "[Episode 700] - Average count of rewards is 61.25.\n",
      "[Episode 750] - Average count of rewards is 55.09.\n",
      "[Episode 800] - Average count of rewards is 55.57.\n",
      "[Episode 850] - Average count of rewards is 57.28.\n",
      "[Episode 900] - Average count of rewards is 61.85.\n",
      "[Episode 950] - Average count of rewards is 63.91.\n",
      "Ran 954 episodes. Solved after 949 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "agent = MountCarSolver_SARSA()\n",
    "agent.run()\n",
    "del agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completes about 50% of the time.  Can complete at iteration 300 or close to the end.  Completion is defined as reaching the goal 5 times in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
